
First we will estimate a linear regression of salary midpoints on each set of predictors. Therefore this model is parameterized succinctly by the number of salary brackets we use to generate the protowords in the PU model. We will then estimate three categorical response models: an ordinal generalized linear model (a discriminative model), a naive Bayes classifier (a generative model), and a decision tree. For parsimony and simplicity of interpetation, we will use the same salary categories as in the corresponding PU model. Therefore the categorical models are more dependent than the continuous model on the number of categories we choose. Moreover, quantizing or binning the response without good reason amounts to throwing away information, but it is not that clear the full range of variation in salary contains **useful** information. In this case, binning the response variable could produce a better model by focusing on broader patterns of association, rather than fitting a model to variation that we know we probably cannot explain with our data.

Each model takes a different conceptual approach. Linear regression, even when its classical foundations fail, always produces a first-order approximation of association between the response and the predictors. Among the categorical response models, the ordinal GLM is the closest analogue to linear regression, in that the ordering of the response is preserved and the model parameters can be cleanly interpreted. The naive Bayes classifier abandons ordinality, which would make sense if the predictors of high-wage jobs are not ordinally "larger" than the predictors of low-wage jobs, as our PU setup allows for and in some sense imposes. Finally, the decision tree abandons linearity in the predictors (naive Bayes is linear in log-probability space): there is no obvious reason why the model should be linear and the decision tree is conceptually our "most general."

Our goal is to determine which characteristics of a job posting, if any, are meaningful predictors of wage. High predictive accuracy would suggest highly informative predictors, and in this case a highly informative predictor is in fact a meaningful one. Therefore we will attempt to build a model with high predictive accuracy, and our selection criteria will be average prediction error and average log predictive density on the test data. We will also compare models pairwise using posterior odds ratios. Posterior odds ratios are sensitive to untestable model assumptions that are common between both models, but in non-nested models case we are explicitly comparing those untestable assumptions and therefore posterior odds are actually an excellent criterion for our purposes.
